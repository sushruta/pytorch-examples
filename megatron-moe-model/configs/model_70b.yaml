# MoE - 70B+ model (16+ GPUs)
model:
  name: "mymodel-moe-70b"
  vocab_size: 32000
  hidden_size: 8192
  num_layers: 60
  num_attention_heads: 64
  num_key_value_heads: 8  # GQA for memory efficiency
  intermediate_size: 22016  # 2.688 * hidden_size
  max_position_embeddings: 4096

  # MoE configuration
  num_experts: 8
  num_experts_per_tok: 2
  moe_layer_indices: [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55]  # ~18% of layers are MoE
  router_aux_loss_coef: 0.01
  router_z_loss_coef: 0.001

  # Architecture details
  rms_norm_eps: 1.0e-6
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  use_flash_attention: true

training:
  batch_size: 2
  sequence_length: 4096
  gradient_accumulation_steps: 16
  learning_rate: 1.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  max_steps: 100000

  # Precision
  precision: "bf16"
  use_torch_compile: true

  # Distributed (16 GPUs)
  distributed: true
  expert_parallel_size: 4  # 2 experts per GPU
  data_parallel_size: 4

  # Logging
  log_interval: 500
  eval_interval: 1000
